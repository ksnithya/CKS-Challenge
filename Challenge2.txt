Question:
A persistentVolume called 'alpha-pv' has already been created. Do not modify it and inspect the parameters used to create it.

Solution:
root@controlplane ~ ➜  k get pv -n alpha
NAME       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS    REASON   AGE
alpha-pv   1Gi        RWX            Delete           Available           local-storage            3m35s


root@controlplane ~ ✖ k get pv -n alpha -o yaml
apiVersion: v1
items:
- apiVersion: v1
  kind: PersistentVolume
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"PersistentVolume","metadata":{"annotations":{},"name":"alpha-pv"},"spec":{"accessModes":["ReadWriteMany"],"capacity":{"storage":"1Gi"},"local":{"path":"/data/pages"},"nodeAffinity":{"required":{"nodeSelectorTerms":[{"matchExpressions":[{"key":"kubernetes.io/hostname","operator":"In","values":["controlplane"]}]}]}},"persistentVolumeReclaimPolicy":"Delete","storageClassName":"local-storage","volumeMode":"Filesystem"}}
    creationTimestamp: "2023-03-21T06:34:43Z"
    finalizers:
    - kubernetes.io/pv-protection
    name: alpha-pv
    resourceVersion: "583"
    uid: 70d1d2b6-8a38-49eb-84e4-0a096a2aa15d
  spec:
    accessModes:
    - ReadWriteMany
    capacity:
      storage: 1Gi
    local:
      path: /data/pages
    nodeAffinity:
      required:
        nodeSelectorTerms:
        - matchExpressions:
          - key: kubernetes.io/hostname
            operator: In
            values:
            - controlplane
    persistentVolumeReclaimPolicy: Delete
    storageClassName: local-storage
    volumeMode: Filesystem
  status:
    phase: Available
kind: List
metadata:
  resourceVersion: ""
  selfLink: ""
----------------------------------------------------
Question:

'alpha-pvc' should be bound to 'alpha-pv'. Delete and Re-create it if necessary.

Solution:
root@controlplane ~ ➜  k get pv -n alpha
NAME       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS    REASON   AGE
alpha-pv   1Gi        RWX            Delete           Available           local-storage            7m11s

root@controlplane ~ ➜  k get pvc -n alpha
NAME        STATUS    VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS    AGE
alpha-pvc   Pending                                      local-storage   7m22s

root@controlplane ~ ✖ k get pvc -n alpha -o yaml>alpha-pvc.yml

root@controlplane ~ ➜  vi alpha-pvc.yml 

root@controlplane ~ ➜  k replace -f alpha-pvc.yml --force
persistentvolumeclaim "alpha-pvc" deleted
persistentvolumeclaim/alpha-pvc replaced

root@controlplane ~ ➜  k get pvc -n alpha 
NAME        STATUS   VOLUME     CAPACITY   ACCESS MODES   STORAGECLASS    AGE
alpha-pvc   Bound    alpha-pv   1Gi        RWX            local-storage   10s

root@controlplane ~ ➜  k get pv -n alpha 
NAME       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM             STORAGECLASS    REASON   AGE
alpha-pv   1Gi        RWX            Delete           Bound    alpha/alpha-pvc   local-storage            11m

------------------------------------------------------

There are 6 images listed in the diagram on the right. Using Aquasec Trivy (which is already installed on the controlplane node), identify the image that has the least number of critical vulnerabilities and use it to deploy the alpha-xyz deployment.

Secure this deployment by enforcing the AppArmor profile called custom-nginx.

Expose this deployment with a NodePort type service and make sure that only incomings connections from the pod called middleware is accepted and everything else is rejected.

Click on each icon to see more details. Once done, click the Check button to test your work.
--------------

Create a deployment called 'alpha-xyz' that uses the image with the least 'CRITICAL' vulnerabilities? (Use the sample YAML file located at '/root/alpha-xyz.yaml' to create the deployment. 
Please make sure to use the same names and labels specified in this sample YAML file!)

Deployment has exactly '1' ready replica

'data-volume' is mounted at '/usr/share/nginx/html' on the pod

Solution:
root@controlplane ~ ➜  docker images|grep -i nginx
bitnami/nginx                        latest              c6f53c626073        2 days ago          94.5MB
nginx                                alpine              2bc7edbc3cf2        5 weeks ago         40.7MB
nginx                                <none>              7d73f57a7cf7        12 months ago       23.4MB
nginx                                latest              f2f70adc5d89        12 months ago       142MB
nginx                                1.17                9beeba249f3e        2 years ago         127MB
nginx                                1.16                dfcfd8e9a5d3        2 years ago         127MB
nginx                                1.14                295c7be07902        3 years ago         109MB
nginx                                1.13                ae513a47849c        4 years ago         109MB

root@controlplane ~ ➜  trivy image -s CRITICAL nginx:1.16|grep -i critical
Total: 43 (CRITICAL: 43)
| dpkg         | CVE-2022-1664    | CRITICAL | 1.19.7                    | 1.19.8                  | Dagster-cloud 1.1.4 updates             |

root@controlplane ~ ➜  trivy image -s CRITICAL nginx:1.14|grep -i critical
Total: 64 (CRITICAL: 64)
| dpkg              | CVE-2022-1664    | CRITICAL | 1.18.25                | 1.18.26                | Dagster-cloud 1.1.4 updates             |

root@controlplane ~ ➜  trivy image -s CRITICAL nginx:1.13|grep -i critical
Total: 85 (CRITICAL: 85)
| dpkg              | CVE-2022-1664    | CRITICAL | 1.18.24                | 1.18.26                | Dagster-cloud 1.1.4 updates             |

root@controlplane ~ ➜  trivy image -s CRITICAL nginx:alpine|grep -i critical
Total: 0 (CRITICAL: 0)

root@controlplane ~ ➜  trivy image -s CRITICAL nginx:1.17|grep -i critical
Total: 43 (CRITICAL: 43)
| dpkg         | CVE-2022-1664    | CRITICAL | 1.19.7                    | 1.19.8                  | Dagster-cloud 1.1.4 updates             |

root@controlplane ~ ➜  trivy image -s CRITICAL bitnami/nginx:latest|grep -i critical
Total: 3 (CRITICAL: 3)
| libc-bin | CVE-2019-1010022 | CRITICAL | 2.31-13+deb11u5   |               | glibc: stack guard protection bypass    |

root@controlplane ~ ➜  

root@controlplane ~ ➜  
root@controlplane ~ ➜  k create -f alpha-xyz.yaml 
deployment.apps/alpha-xyz created

root@controlplane ~ ➜  k get pod -n alpha
NAME                         READY   STATUS    RESTARTS   AGE
alpha-xyz-75ddb5d4b6-hxbwl   1/1     Running   0          8s
external                     1/1     Running   0          32m
middleware                   1/1     Running   0          32m

root@controlplane ~ ➜  cat alpha-xyz.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  creationTimestamp: null
  labels:
    app: alpha-xyz
  name: alpha-xyz
  namespace: alpha
spec:
  replicas: 1
  selector:
    matchLabels:
      app: alpha-xyz
  strategy: {}
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: alpha-xyz
    spec:
      containers:
      - image: nginx:alpine
        name: nginx
        volumeMounts:
          - name: data-volume
            mountPath: /usr/share/nginx/html
      volumes:
        - name: data-volume
          persistentVolumeClaim:
            claimName: alpha-pvc
---------------------------------------------------

Move the AppArmor profile '/root/usr.sbin.nginx' to '/etc/apparmor.d/usr.sbin.nginx' on the controlplane node

Load the 'AppArmor` profile called 'custom-nginx' and ensure it is enforced.


root@controlplane ~ ➜  ls -l
total 12
-rw-r--r-- 1 root root  340 Mar 21 06:45 alpha-pvc.yml
-rw-rw-rw- 1 root root  382 Mar 14 09:54 alpha-xyz.yaml
-rw-rw-rw- 1 root root 1307 Mar 14 09:54 usr.sbin.nginx

root@controlplane ~ ➜  cp usr.sbin.nginx /etc/apparmor.d/

root@controlplane ~ ➜  apparmor_parser -a /etc/apparmor.d/usr.sbin.nginx

root@controlplane ~ ➜  apparmor_status |grep nginx
   custom-nginx
-----------------------------------------------

Create a NetworkPolicy called 'restrict-inbound' in the 'alpha' namespace

Policy Type = 'Ingress'

Inbound access only allowed from the pod called 'middleware' with label 'app=middleware'

Inbound access only allowed to TCP port 80 on pods matching the policy

Solution:
root@controlplane ~ ➜  k get po -n alpha --show-labels
NAME                         READY   STATUS    RESTARTS   AGE     LABELS
alpha-xyz-75ddb5d4b6-hxbwl   1/1     Running   0          6m42s   app=alpha-xyz,pod-template-hash=75ddb5d4b6
external                     1/1     Running   0          38m     app=external
middleware                   1/1     Running   0          38m     app=middleware

root@controlplane ~ ➜  k create -f net.yml 
networkpolicy.networking.k8s.io/netpolicy created

root@controlplane ~ ➜  k get netpol -n alpha
NAME        POD-SELECTOR    AGE
netpolicy   app=alpha-xyz   7s

root@controlplane ~ ➜  cat net.yml 
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: netpolicy
  namespace: alpha
spec:
  podSelector:
    matchLabels:
      app: alpha-xyz
  policyTypes:
    - Ingress
  ingress:
    - from:
        - podSelector:
            matchLabels:
              app: middleware
      ports:
        - protocol: TCP
          port: 80

root@controlplane ~ ➜  

----------------------------------------
Expose the 'alpha-xyz' as a 'ClusterIP' type service called 'alpha-svc'

'alpha-svc' should be exposed on 'port: 80' and 'targetPort: 80'

root@controlplane ~ ➜  k expose deploy alpha-xyz -n alpha --port=80 --target-port=80 --name=alpha-svc
service/alpha-svc exposed

root@controlplane ~ ➜  k get svc -n alpha
NAME        TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE
alpha-svc   ClusterIP   10.101.166.74   <none>        80/TCP    6s

root@controlplane ~ ➜  

--------------------
'alpha-xyz' deployment uses the 'custom-nginx' apparmor profile (applied to container called 'nginx')

root@controlplane ~ ➜  k get deploy -n alpha -o yaml
apiVersion: v1
items:
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "2"
    creationTimestamp: "2023-03-21T07:07:09Z"
    generation: 2
    labels:
      app: alpha-xyz
    name: alpha-xyz
    namespace: alpha
    resourceVersion: "4684"
    uid: 0233cc05-4e20-43a6-91d8-9f68be7b51ba
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: alpha-xyz
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        annotations:
          container.apparmor.security.beta.kubernetes.io/nginx: localhost/custom-nginx
        creationTimestamp: null
        labels:
          app: alpha-xyz
      spec:
        containers:
        - image: nginx:alpine
          imagePullPolicy: IfNotPresent
          name: nginx
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /usr/share/nginx/html
            name: data-volume
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - name: data-volume
          persistentVolumeClaim:
            claimName: alpha-pvc
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2023-03-21T07:07:12Z"
      lastUpdateTime: "2023-03-21T07:07:12Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2023-03-21T07:07:09Z"
      lastUpdateTime: "2023-03-21T07:30:18Z"
      message: ReplicaSet "alpha-xyz-68cbd57f9" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 2
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
kind: List
metadata:
  resourceVersion: ""
  selfLink: ""

root@controlplane ~ ➜  kubectl create -f net.yml 
networkpolicy.networking.k8s.io/prod-netpol created

root@controlplane ~ ➜  kubectl get netpol -n prod
NAME          POD-SELECTOR   AGE
prod-netpol   <none>         8s

root@controlplane ~ ➜  